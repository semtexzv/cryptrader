   0.0 TEL | Telepresence 0.98-11-ga30a3e4 launched at Wed May  8 22:45:20 2019
   0.0 TEL |   /usr/bin/telepresence
   0.0 TEL |   Using images version 0.98 (dev)
   0.0 TEL | Platform: linux
   0.0 TEL | Python 3.7.3 (default, Mar 26 2019, 21:43:19)
   0.0 TEL | [GCC 8.2.1 20181127]
   0.0 TEL | [1] Running: uname -a
   0.0   1 | Linux semtexzv-pc 5.0.10-arch1-1-ARCH #1 SMP PREEMPT Sat Apr 27 20:06:45 UTC 2019 x86_64 GNU/Linux
   0.0 TEL | [1] ran in 0.00 secs.
   0.0 TEL | BEGIN SPAN main.py:40(main)
   0.0 TEL | BEGIN SPAN startup.py:74(__init__)
   0.0 TEL | Found kubectl -> /usr/bin/kubectl
   0.0 TEL | [2] Capturing: kubectl version --short
   0.4 TEL | [2] captured in 0.41 secs.
   0.4 TEL | [3] Capturing: kubectl config current-context
   0.7 TEL | [3] captured in 0.28 secs.
   0.7 TEL | [4] Capturing: kubectl config view -o json
   0.8 TEL | [4] captured in 0.08 secs.
   0.8 TEL | [5] Capturing: kubectl --context do-fra1-main get ns default
   1.6 TEL | [5] captured in 0.77 secs.
   1.6 TEL | Command: kubectl 1.14.1
   1.6 TEL | Context: do-fra1-main, namespace: default, version: 1.13.5
   1.6 TEL | END SPAN startup.py:74(__init__)    1.5s
   1.6 TEL | Found ssh -> /usr/bin/ssh
   1.6 TEL | [6] Capturing: ssh -V
   1.6 TEL | [6] captured in 0.01 secs.
   1.6 TEL | Found bash -> /usr/bin/bash
   1.6 TEL | Found sshuttle-telepresence -> /usr/libexec/sshuttle-telepresence
   1.6 TEL | Found conntrack -> /usr/bin/conntrack
   1.6 TEL | Found iptables -> /usr/bin/iptables
   1.6 TEL | Found sudo -> /usr/bin/sudo
   1.6 TEL | [7] Running: sudo -n echo -n
   1.6   7 | sudo: a password is required
   1.6 TEL | [7] exit 1 in 0.02 secs.
   1.6 >>> | Invoking sudo. Please enter your sudo password.
   1.6 TEL | [8] Running: sudo echo -n
   6.2 TEL | [8] ran in 4.60 secs.
   6.2 >>> | Starting proxy with method 'vpn-tcp', which has the following limitations: All processes are affected, only one telepresence can run per machine, and you can't use other VPNs. You may need to add cloud hosts and headless services with --also-proxy. For a full list of method limitations see https://telepresence.io/reference/methods.html
   6.2 TEL | Found sshfs -> /usr/bin/sshfs
   6.2 TEL | Found fusermount -> /usr/bin/fusermount
   6.2 >>> | Volumes are rooted at $TELEPRESENCE_ROOT. See https://telepresence.io/howto/volumes.html for details.
   6.2 TEL | [9] Running: kubectl --context do-fra1-main --namespace default get pods telepresence-connectivity-check --ignore-not-found
   6.8 TEL | [9] ran in 0.58 secs.
   7.6 TEL | Scout info: {'latest_version': '0.99', 'application': 'telepresence', 'notices': []}
   7.6 TEL | BEGIN SPAN deployment.py:70(create_new_deployment)
   7.6 >>> | Starting network proxy to cluster using new Deployment telepresence-1557348320-6926117-18807
   7.6 TEL | [10] Running: kubectl --context do-fra1-main --namespace default delete --ignore-not-found svc,deploy --selector=telepresence=fb186e5a5f7740e5b9b026b0a8f6d33f
   8.0  10 | No resources found
   8.0 TEL | [10] ran in 0.39 secs.
   8.0 TEL | [11] Running: kubectl --context do-fra1-main --namespace default run --restart=Always --limits=cpu=100m,memory=256Mi --requests=cpu=25m,memory=64Mi telepresence-1557348320-6926117-18807 --image=datawire/telepresence-k8s:0.98 --labels=telepresence=fb186e5a5f7740e5b9b026b0a8f6d33f
   8.6  11 | kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
   8.7  11 | deployment.apps/telepresence-1557348320-6926117-18807 created
   8.7 TEL | [11] ran in 0.67 secs.
   8.7 TEL | END SPAN deployment.py:70(create_new_deployment)    1.1s
   8.7 TEL | BEGIN SPAN remote.py:151(get_remote_info)
   8.7 TEL | BEGIN SPAN remote.py:78(get_deployment_json)
   8.7 TEL | [12] Capturing: kubectl --context do-fra1-main --namespace default get deployment -o json --selector=telepresence=fb186e5a5f7740e5b9b026b0a8f6d33f
   9.2 TEL | [12] captured in 0.50 secs.
   9.2 TEL | END SPAN remote.py:78(get_deployment_json)    0.5s
   9.2 TEL | Searching for Telepresence pod:
   9.2 TEL |   with name telepresence-1557348320-6926117-18807-*
   9.2 TEL |   with labels {'telepresence': 'fb186e5a5f7740e5b9b026b0a8f6d33f'}
   9.2 TEL | [13] Capturing: kubectl --context do-fra1-main --namespace default get pod -o json --selector=telepresence=fb186e5a5f7740e5b9b026b0a8f6d33f
   9.7 TEL | [13] captured in 0.48 secs.
   9.7 TEL | Checking telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh
   9.7 TEL | Looks like we've found our pod!
   9.7 TEL | BEGIN SPAN remote.py:113(wait_for_pod)
   9.7 TEL | [14] Capturing: kubectl --context do-fra1-main --namespace default get pod telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh -o json
  10.1 TEL | [14] captured in 0.35 secs.
  10.3 TEL | [15] Capturing: kubectl --context do-fra1-main --namespace default get pod telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh -o json
  10.8 TEL | [15] captured in 0.45 secs.
  11.0 TEL | [16] Capturing: kubectl --context do-fra1-main --namespace default get pod telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh -o json
  11.4 TEL | [16] captured in 0.36 secs.
  11.6 TEL | [17] Capturing: kubectl --context do-fra1-main --namespace default get pod telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh -o json
  12.0 TEL | [17] captured in 0.42 secs.
  12.3 TEL | [18] Capturing: kubectl --context do-fra1-main --namespace default get pod telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh -o json
  12.9 TEL | [18] captured in 0.58 secs.
  13.1 TEL | [19] Capturing: kubectl --context do-fra1-main --namespace default get pod telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh -o json
  13.6 TEL | [19] captured in 0.50 secs.
  13.9 TEL | [20] Capturing: kubectl --context do-fra1-main --namespace default get pod telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh -o json
  14.3 TEL | [20] captured in 0.42 secs.
  14.5 TEL | [21] Capturing: kubectl --context do-fra1-main --namespace default get pod telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh -o json
  15.0 TEL | [21] captured in 0.42 secs.
  15.2 TEL | [22] Capturing: kubectl --context do-fra1-main --namespace default get pod telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh -o json
  15.7 TEL | [22] captured in 0.51 secs.
  16.0 TEL | [23] Capturing: kubectl --context do-fra1-main --namespace default get pod telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh -o json
  16.4 TEL | [23] captured in 0.42 secs.
  16.7 TEL | [24] Capturing: kubectl --context do-fra1-main --namespace default get pod telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh -o json
  17.0 TEL | [24] captured in 0.39 secs.
  17.3 TEL | [25] Capturing: kubectl --context do-fra1-main --namespace default get pod telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh -o json
  17.9 TEL | [25] captured in 0.57 secs.
  18.1 TEL | [26] Capturing: kubectl --context do-fra1-main --namespace default get pod telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh -o json
  18.8 TEL | [26] captured in 0.69 secs.
  18.8 TEL | END SPAN remote.py:113(wait_for_pod)    9.1s
  18.8 TEL | END SPAN remote.py:151(get_remote_info)   10.1s
  18.8 TEL | BEGIN SPAN connect.py:36(connect)
  18.8 TEL | [27] Launching kubectl logs: kubectl --context do-fra1-main --namespace default logs -f telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh --container telepresence-1557348320-6926117-18807 --tail=10
  18.8 TEL | [28] Launching kubectl port-forward: kubectl --context do-fra1-main --namespace default port-forward telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh 43141:8022
  18.8 TEL | [29] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 43141 telepresence@127.0.0.1 /bin/true
  18.9 TEL | [29] exit 255 in 0.05 secs.
  19.1 TEL | [30] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 43141 telepresence@127.0.0.1 /bin/true
  19.2 TEL | [30] exit 255 in 0.06 secs.
  19.4 TEL | [31] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 43141 telepresence@127.0.0.1 /bin/true
  19.5 TEL | [31] exit 255 in 0.03 secs.
  19.6  28 | Forwarding from 127.0.0.1:43141 -> 8022
  19.6  28 | Forwarding from [::1]:43141 -> 8022
  19.7 TEL | [32] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 43141 telepresence@127.0.0.1 /bin/true
  19.7  28 | Handling connection for 43141
  20.7 TEL | [32] ran in 1.01 secs.
  20.7 >>> | 
  20.7 >>> | No traffic is being forwarded from the remote Deployment to your local machine. You can use the --expose option to specify which ports you want to forward.
  20.7 >>> | 
  20.7 TEL | Launching Web server for proxy poll
  20.7 TEL | [33] Launching SSH port forward (socks and proxy poll): ssh -N -oServerAliveInterval=1 -oServerAliveCountMax=10 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -q -p 43141 telepresence@127.0.0.1 -L127.0.0.1:40437:127.0.0.1:9050 -R9055:127.0.0.1:44891
  20.8 TEL | END SPAN connect.py:36(connect)    1.9s
  20.8 TEL | BEGIN SPAN remote_env.py:28(get_remote_env)
  20.8 TEL | [34] Capturing: kubectl --context do-fra1-main --namespace default exec telepresence-1557348320-6926117-18807-69bfbd8986-6gjgh --container telepresence-1557348320-6926117-18807 -- python3 podinfo.py
  20.8  28 | Handling connection for 43141
  24.2 TEL | [34] captured in 3.44 secs.
  24.2 TEL | END SPAN remote_env.py:28(get_remote_env)    3.4s
  24.2 TEL | BEGIN SPAN mount.py:32(mount_remote_volumes)
  24.2 TEL | [35] Running: sshfs -p 43141 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null telepresence@127.0.0.1:/ /tmp/tel-f6cy5dnr/fs
  24.3  28 | Handling connection for 43141
